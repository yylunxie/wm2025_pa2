import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Ë®≠ÂÆö‰ΩøÁî®ÁöÑ GPU Á∑®Ëôü
import torch
import numpy as np
import pandas as pd
import random
import argparse
from tqdm import tqdm
from utils import mapk
import torch.utils.data as data
from model import BPRMF
from dataset import PositivePairDataset
from collections import defaultdict


def generate_submission(model, dataset, train_data, output_path, device, top_k=50, rec_pool=1000):
    model.eval()
    user_ids = sorted(dataset.keys())
    all_predictions = []

    with torch.no_grad():
        for user in tqdm(user_ids):
            user_tensor = torch.LongTensor([user]).to(device)
            rec_items = model.recommend(user_tensor, top_k=rec_pool)[0].tolist()

            seen_items = set(train_data[user])
            final_rec = [i for i in rec_items if i not in seen_items][:top_k]

            # ÈÅøÂÖç‰∏çË∂≥ 50 ÂÄã
            if len(final_rec) < top_k:
                print(f"‚ö†Ô∏è User {user} only has {len(final_rec)} unseen predictions.")
                padding_item = max([i for items in train_data.values() for i in items], default=0) + 1
                while len(final_rec) < top_k:
                    final_rec.append(padding_item)

            all_predictions.append({
                "UserId": user,
                "ItemId": " ".join(map(str, final_rec))
            })

    pd.DataFrame(all_predictions).to_csv(output_path, index=False)
    print(f"‚úÖ Submission saved to {output_path}")


def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    np.random.seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    
def split_data(user_items, ratio):
    train_data = {}
    valid_data = {}

    for user, items in user_items.items():
        if len(items) < 2:
            train_data[user] = items
            valid_data[user] = []
            continue
        split_point = int(len(items) * ratio)
        # split_point = -10
        train_data[user] = items[:split_point]
        valid_data[user] = items[split_point:]
    return train_data, valid_data
            

def evaluate_model(model, train_data, val_data, device, top_k=50):
    model.eval()
    all_preds = []
    all_truth = []

    with torch.no_grad():
        for user in sorted(val_data.keys()):
            true_items = val_data[user]
            if not true_items:
                continue  # ÁÑ°È©óË≠âË≥áÊñôÁöÑ user Ë∑≥ÈÅé

            user_tensor = torch.LongTensor([user]).to(device)
            rec_items = model.recommend(user_tensor, top_k=top_k)[0].tolist()

            seen_items = set(train_data[user])
            final_rec = [i for i in rec_items if i not in seen_items][:top_k]

            # Ë£úÊªøÊé®Ëñ¶Èï∑Â∫¶ÈÅøÂÖçÂ†±ÈåØ
            while len(final_rec) < top_k:
                final_rec.append(0)

            all_preds.append(final_rec)
            all_truth.append(true_items)

    return mapk(all_truth, all_preds, k=top_k)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_path", type=str, default="train.csv")
    parser.add_argument("--output_path", type=str, default="output/bpr_submission.csv")
    parser.add_argument("--mode", type=str, default="val", choices=["train", "val"])
    parser.add_argument("--loss", type=str, default="bpr", choices=["bpr", "bce"])
    parser.add_argument("--embedding_dim", type=int, default=64)
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--neg", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=1024)
    parser.add_argument("--lr", type=float, default=1e-3)
    args = parser.parse_args()
    
    best_score = 0
    best_epoch = -1
    
    set_seed(42)
    save_dir = f"weights/d{args.embedding_dim}"
    os.makedirs(save_dir, exist_ok=True)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Initialize the dataset
    user_items = defaultdict(list)
    user_set = set()
    item_set = set()
    # Read the train.csv file
    # train.csv ÁöÑÊ†ºÂºèÊòØ user_id, item_id1 item_id2 item_id3 ...
    
    with open(args.train_path, 'r') as f:
        next(f)  # Ë∑≥ÈÅéÁ¨¨‰∏ÄË°åÁöÑË°®È†≠
        for line in f:
            parts = line.strip().split(',')
            user = int(parts[0])
            items = list(map(int, parts[1].split()))
            user_items[user] = items
            user_set.add(user)
            item_set.update(items)
    all_items = list(item_set)
    train_data, valid_data = split_data(user_items, 0.8)
    train_complement = {user: [item for item in all_items if item not in items] for user, items in train_data.items()}
    # train_complement ÊòØ‰∏ÄÂÄãÂ≠óÂÖ∏Ôºåkey ÊòØ user idÔºåvalue ÊòØË©≤ user Ê≤íÊúâÁúãÈÅéÁöÑ item id list
    valid_complement = {user: [item for item in all_items if item not in items] for user, items in valid_data.items()}
    dataset = PositivePairDataset(train_data)
    dataloader = data.DataLoader(dataset, batch_size=args.batch_size)

    # dataset ÊúÉÂ∞á train.csv ËÆÄÈÄ≤‰æÜ‰∏¶ÂÅöË≥áÊñôÂâçËôïÁêÜÔºå
    # ÂåÖÂê´ train_data, valid_data, user_items, user_set, item_set
    # train_data ÊòØ‰∏ÄÂÄãÂ≠óÂÖ∏Ôºåkey ÊòØ user idÔºåvalue ÊòØË©≤ user ÁúãÈÅéÁöÑ item id list
    # valid_data ÊòØ‰∏ÄÂÄãÂ≠óÂÖ∏Ôºåkey ÊòØ user idÔºåvalue ÊòØË©≤ user Âú®È©óË≠âÈõÜ‰∏äÁúãÈÅéÁöÑ item id list
    # user_items ÊòØ‰∏ÄÂÄãÂ≠óÂÖ∏Ôºåkey ÊòØ user idÔºåvalue ÊòØË©≤ user ÁúãÈÅéÁöÑ item id list
    # user_set Âíå item_set ÊòØÊâÄÊúâ user Âíå item ÁöÑÈõÜÂêà
    num_users = max(user_set) + 1
    num_items = max(item_set) + 1
    
    if args.mode == "train":
        model = BPRMF(num_users, num_items, embedding_dim=args.embedding_dim).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
        for epoch in range(args.epochs):
            total_loss = 0
            for batch_users, batch_pos_items in tqdm(dataloader, desc=f"Epoch {epoch+1}"):
            # for batch in dataloader:
                batch_users = batch_users.to(device)
                batch_pos_items = batch_pos_items.to(device)
                
                triplet_users = []
                triplet_pos = []
                triplet_neg = []
                
                for i in range(len(batch_users)):
                    u = batch_users[i].item()
                    pos = batch_pos_items[i].item()
                    
                    seen_items = set(user_items[u])
                    candidates = [item for item in all_items if item not in seen_items]
                    if len(candidates) == 0:
                        continue
                    candidates = random.sample(candidates, min(100, len(candidates)))
                    
                    u_tensor = torch.LongTensor([u] * len(candidates)).to(device)  # [C]
                    item_tensor = torch.LongTensor(candidates).to(device)          # [C]
                    
                    # candidate = random.sample(train_complement[int(user[i])], 200)
                    scores = model(u_tensor, item_tensor).detach()  # [C]
                    top_indices = torch.topk(scores, k=min(10, len(candidates))).indices
                    hard_negatives = [candidates[idx] for idx in top_indices.tolist()]
                    final_negs = random.sample(hard_negatives, min(args.neg, len(hard_negatives)))
                    for neg in final_negs:
                        triplet_users.append(u)
                        triplet_pos.append(pos)
                        triplet_neg.append(neg)
                        
                if not triplet_users:
                    continue
                
                u_tensor = torch.LongTensor(triplet_users).to(device)
                i_tensor = torch.LongTensor(triplet_pos).to(device)
                j_tensor = torch.LongTensor(triplet_neg).to(device)
                
                if args.loss == "bce":
                    # BCE loss
                    u_emb = model.user_embedding(torch.cat([u_tensor, u_tensor]))
                    item_emb = model.item_embedding(torch.cat([i_tensor, j_tensor]))
                    labels = torch.cat([
                        torch.ones(len(i_tensor)),
                        torch.zeros(len(j_tensor))
                    ]).to(device)
                    scores = (u_emb * item_emb).sum(dim=1)
                    loss = torch.nn.functional.binary_cross_entropy_with_logits(scores, labels)
                elif args.loss == "bpr":
                    # BPR loss
                    u_emb = model.user_embedding(u_tensor)
                    i_emb = model.item_embedding(i_tensor)
                    j_emb = model.item_embedding(j_tensor)
                    
                    pos_scores = (u_emb * i_emb).sum(dim=1)
                    neg_scores = (u_emb * j_emb).sum(dim=1)
                    
                    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()
                # u_emb = model.user_embedding(u_tensor)
                # i_emb = model.item_embedding(i_tensor)
                # j_emb = model.item_embedding(j_tensor)
                
                # pos_scores = (u_emb * i_emb).sum(dim=1)
                # neg_scores = (u_emb * j_emb).sum(dim=1)
                
                # loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                
            print(f"[Epoch {epoch+1}] Loss: {total_loss / len(dataloader):.4f}")

            val_map = evaluate_model(model, val_data=valid_data, train_data=train_data, device=device)
            print(f"[Validation MAP@50]: {val_map:.4f}")

            if val_map > best_score:
                best_score = val_map
                best_epoch = epoch
                model_path = f"weights/{args.loss}/d{args.embedding_dim}/best_model_{args.loss}.pt"
                torch.save(model.state_dict(), model_path)
                print(f"‚úÖ New best model saved at epoch {epoch+1} with MAP@50 = {val_map:.4f}")
    # elif args.mode == "val":
    model = BPRMF(num_users, num_items, embedding_dim=args.embedding_dim).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
        
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    
    generate_submission(model, user_items, train_data=train_data, output_path=args.output_path, device=device)
    print(f"üèÅ Best MAP@50 = {best_score:.4f} at epoch {best_epoch+1}")
    # Log results
    log_file_path = f"dimension_results_{args.loss}.csv"
    with open(log_file_path, "a") as f:
        if not os.path.exists(log_file_path) or os.path.getsize(log_file_path) == 0:
            f.write("embedding_dim,best_map@50,best_epoch\n")
        f.write(f"{args.embedding_dim},{best_score:.4f},{best_epoch+1}\n")
    
    print(f"Results for dimension {args.embedding_dim} added to {log_file_path}")
    

if __name__ == "__main__":
    main()